{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "from datetime import datetime\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import IterativeImputer, SimpleImputer\n",
    "\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import KFold"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Loading the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"./dataset.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all columns selection\n",
    "columns = list(data.columns)\n",
    "columns.remove(\"y\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_X = data[columns]\n",
    "data_y = data[[\"y\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Train Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(data_X.values, \n",
    "                                                 data_y.values.ravel(),\n",
    "                                                 test_size=0.3,\n",
    "                                                 shuffle=True,\n",
    "                                                 random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "1. Remove x067, x094, x095, x096 - constant columns\n",
    "2. Remove highly correlated binary variables - x246, x247, x261, x262, x270, x271, x282, x284, x300\n",
    "3. Do the following mapping on categoprical or ordinal variables - ```\n",
    "Categorical variables mapping:\n",
    "\n",
    "x068 => {>=1 : 1}\n",
    "x077 => {>=1 : 1}\n",
    "x078 => {>=1 : 1}\n",
    "x079 => {>=1 : 1}\n",
    "x156 => {>=1 : 1}\n",
    "x252 => {>=1 : 1}\n",
    "[67, 76, 77, 78, 155, 151]\n",
    "\n",
    "x037 => {>=2 : 2}\n",
    "x049 => {>=2 : 2}\n",
    "x050 => {>=2 : 2}\n",
    "x051 => {>=2 : 2}\n",
    "x052 => {>=2 : 2}\n",
    "x053 => {>=2 : 2}\n",
    "x107 => {>=2 : 2}\n",
    "[36, 48, 49, 50, 51, 52, 106]\n",
    "\n",
    "x023 => {>=3 : 3}\n",
    "x155 => {>=3 : 3}\n",
    "x252 => {>=3 : 3}\n",
    "[22, 154, 251, 37, 38, 45, 46, 47, 53, 54, 60, 68, 79, 99, 100, 101, 107, 111, 121, 122, 148, 162, 168, 173, 174, 175, 176, 177, 178, 181, 182, 196, 227, 228, 229, 240, 250, 253]\n",
    "\n",
    "x022 => {>=4 : 4}\n",
    "x148 => {>=4 : 4}\n",
    "x162 => {>=4 : 4}\n",
    "x287 => {>=4 : 4}\n",
    "x302 => {>=4 : 4}\n",
    "[21, 147, 161, 286, 301]\n",
    "\n",
    "x019 => {>=5 : 5}\n",
    "[18]\n",
    "\n",
    "x018 => {>=8 : 8}\n",
    "[17]\n",
    "\n",
    "['x038', 'x039', 'x046', 'x047', 'x048',\n",
    " 'x054', 'x055', 'x061', 'x069', 'x080',\n",
    " 'x100', 'x101', 'x102', 'x108', 'x112',\n",
    " 'x122', 'x123', 'x149', 'x163', 'x169',\n",
    " 'x174', 'x175', 'x176', 'x177', 'x178',\n",
    " 'x179', 'x182', 'x183', 'x197', 'x228',\n",
    " 'x229', 'x230', 'x241', 'x251', 'x254'] => {>=3 : 3}\n",
    " \n",
    "```\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def func_map(x, thresh):\n",
    "    if x < thresh:\n",
    "        return x\n",
    "    else:\n",
    "        return thresh\n",
    "vfunc_map = np.vectorize(func_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomTransform1(BaseEstimator, TransformerMixin):\n",
    "    '''\n",
    "    1. Remove x067, x094, x095, x096 - constant columns\n",
    "    2. Remove highly correlated binary variables - x246, x247, x261, x262, x270, x271, x282, x284, x300\n",
    "    Since using hard coded index, this transformation must be used at first\n",
    "    3. Mapping of categorical varibles as mentioned above\n",
    "    '''\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X, y=None):\n",
    "        X_ = X.copy()\n",
    "        list_1 = [67, 76, 77, 78, 155, 151]\n",
    "        list_2 = [36, 48, 49, 50, 51, 52, 106]\n",
    "        list_3 = [22, 154, 251, 37, 38, 45, 46, \n",
    "                  47, 53, 54, 60, 68, 79, 99, 100, \n",
    "                  101, 107, 111, 121, 122, 148, 162, \n",
    "                  168, 173, 174, 175, 176, 177, 178, \n",
    "                  181, 182, 196, 227, 228, 229, 240, 250, 253]\n",
    "        list_4 = [21, 147, 161, 286, 301]\n",
    "        list_5 = [18]\n",
    "        list_8 = [17]\n",
    "        \n",
    "        X_[:,list_1] = vfunc_map(X_[:,list_1], 1)\n",
    "        X_[:,list_2] = vfunc_map(X_[:,list_2], 2)\n",
    "        X_[:,list_3] = vfunc_map(X_[:,list_3], 3)\n",
    "        X_[:,list_4] = vfunc_map(X_[:,list_4], 4)\n",
    "        X_[:,list_5] = vfunc_map(X_[:,list_5], 5)\n",
    "        X_[:,list_8] = vfunc_map(X_[:,list_8], 8)\n",
    "        \n",
    "        X_ = np.delete(X_, [66, 93, 94, 95, 245, 246, 260, 261, 269, 270, 281, 283, 299], 1)\n",
    "        return X_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomTransform2(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self):\n",
    "        self.imputer = SimpleImputer(verbose=1, strategy=\"most_frequent\")\n",
    "        # self.imputer = IterativeImputer(max_iter=10, random_state=42, verbose=1)\n",
    "    def fit(self, X, y=None):\n",
    "        X_ = X.copy()\n",
    "        self.imputer.fit(X_)\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X, y=None):\n",
    "        X_ = X.copy()\n",
    "        imputed = self.imputer.transform(X_)\n",
    "        return imputed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "regressor = GradientBoostingRegressor(loss='lad', \n",
    "                                      learning_rate=0.08, \n",
    "                                      n_estimators=250, # 200\n",
    "                                      subsample=1.0, \n",
    "                                      criterion='friedman_mse', \n",
    "                                      min_samples_split=2, \n",
    "                                      min_samples_leaf=1, \n",
    "                                      min_weight_fraction_leaf=0.0, \n",
    "                                      max_depth=12,\n",
    "                                      min_impurity_decrease=0.0, \n",
    "                                      min_impurity_split=None, \n",
    "                                      init=None, \n",
    "                                      random_state=42, \n",
    "                                      max_features=250, # 200, log2, sqrt\n",
    "                                      alpha=0.9, \n",
    "                                      verbose=3, \n",
    "                                      max_leaf_nodes=None, \n",
    "                                      warm_start=False, \n",
    "                                      validation_fraction=0.1, \n",
    "                                      n_iter_no_change=2, \n",
    "                                      tol=0.0001, \n",
    "                                      ccp_alpha=0.0)\n",
    "\n",
    "time_stamp = datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "model_name = 'models/model-' + time_stamp + '.pkl'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "1. increase lr\n",
    "2. reduce max_features\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline=Pipeline([('custom1', CustomTransform1()),\n",
    "                      ('imputer1', CustomTransform2()),\n",
    "                      # ('scalar1',StandardScaler()),\n",
    "                      ('regressor', regressor)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # KFold cross validation\n",
    "# kfold = KFold(n_splits=10, random_state=42)\n",
    "# cv_results = cross_val_score(pipeline, \n",
    "#                              data.values[:, :-1], \n",
    "#                              data.values[:, -1], \n",
    "#                              cv=kfold, \n",
    "#                              scoring='mean_squared_error')\n",
    "# print(cv_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Iter       Train Loss   Remaining Time \n",
      "         1          92.7760           23.44m\n",
      "         2          85.7114           23.44m\n",
      "         3          79.2712           23.25m\n",
      "         4          73.4302           23.08m\n",
      "         5          68.0872           23.18m\n",
      "         6          63.2046           23.07m\n",
      "         7          58.7855           22.87m\n",
      "         8          54.7497           23.00m\n",
      "         9          51.0249           22.78m\n",
      "        10          47.6989           22.62m\n",
      "        11          44.6644           22.47m\n",
      "        12          41.8982           22.31m\n",
      "        13          39.3537           22.18m\n",
      "        14          37.0174           22.02m\n",
      "        15          34.8817           21.90m\n",
      "        16          32.9400           21.74m\n",
      "        17          31.1575           21.60m\n",
      "        18          29.5111           21.45m\n",
      "        19          28.0006           21.30m\n",
      "        20          26.6403           21.17m\n",
      "        21          25.3820           21.03m\n",
      "        22          24.2656           20.92m\n",
      "        23          23.2078           20.87m\n",
      "        24          22.2130           20.77m\n",
      "        25          21.2892           20.67m\n",
      "        26          20.4717           20.56m\n",
      "        27          19.7491           20.51m\n",
      "        28          19.0500           20.41m\n",
      "        29          18.3955           20.28m\n",
      "        30          17.8054           20.15m\n",
      "        31          17.2896           20.01m\n",
      "        32          16.8264           19.93m\n",
      "        33          16.3674           19.86m\n",
      "        34          15.9694           19.74m\n",
      "        35          15.6008           19.61m\n",
      "        36          15.2554           19.50m\n",
      "        37          14.8990           19.38m\n",
      "        38          14.6032           19.28m\n",
      "        39          14.3194           19.21m\n",
      "        40          14.0531           19.11m\n",
      "        41          13.7919           19.01m\n",
      "        42          13.5743           18.93m\n",
      "        43          13.3569           18.84m\n",
      "        44          13.1622           18.77m\n",
      "        45          12.9954           18.70m\n",
      "        46          12.8073           18.63m\n",
      "        47          12.6386           18.56m\n",
      "        48          12.5133           18.48m\n",
      "        49          12.3913           18.39m\n",
      "        50          12.2646           18.31m\n",
      "        51          12.1541           18.23m\n",
      "        52          12.0012           18.15m\n",
      "        53          11.9034           18.07m\n",
      "        54          11.8123           18.00m\n",
      "        55          11.7299           17.93m\n",
      "        56          11.6357           17.85m\n",
      "        57          11.5497           17.76m\n",
      "        58          11.4886           17.70m\n",
      "        59          11.4151           17.62m\n",
      "        60          11.3583           17.55m\n",
      "        61          11.2975           17.47m\n",
      "        62          11.2339           17.40m\n",
      "        63          11.1626           17.31m\n",
      "        64          11.1132           17.24m\n",
      "        65          11.0655           17.17m\n",
      "        66          10.9605           17.07m\n",
      "        67          10.8922           16.99m\n",
      "        68          10.8255           16.91m\n",
      "        69          10.7656           16.84m\n",
      "        70          10.7046           16.77m\n",
      "        71          10.6392           16.70m\n",
      "        72          10.5934           16.62m\n",
      "        73          10.5469           16.56m\n",
      "        74          10.5026           16.49m\n",
      "        75          10.4627           16.41m\n",
      "        76          10.4111           16.31m\n",
      "        77          10.3689           16.22m\n",
      "        78          10.3218           16.14m\n",
      "        79          10.2825           16.06m\n",
      "        80          10.2486           15.97m\n",
      "        81          10.2156           15.88m\n",
      "        82          10.1818           15.78m\n",
      "        83          10.1523           15.69m\n",
      "        84          10.1140           15.60m\n",
      "        85          10.0657           15.52m\n",
      "        86          10.0377           15.43m\n",
      "        87          10.0126           15.36m\n",
      "        88           9.9849           15.28m\n",
      "        89           9.9559           15.20m\n",
      "        90           9.9274           15.12m\n",
      "        91           9.8984           15.03m\n",
      "        92           9.8772           14.96m\n",
      "        93           9.8419           14.87m\n",
      "        94           9.8235           14.78m\n",
      "        95           9.7952           14.69m\n",
      "        96           9.7726           14.61m\n",
      "        97           9.7486           14.52m\n",
      "        98           9.7291           14.44m\n",
      "        99           9.7094           14.35m\n",
      "       100           9.6857           14.27m\n",
      "       101           9.6633           14.18m\n",
      "       102           9.6519           14.10m\n",
      "       103           9.6348           14.01m\n",
      "       104           9.6142           13.93m\n",
      "       105           9.5833           13.84m\n",
      "       106           9.5610           13.75m\n",
      "       107           9.5373           13.66m\n",
      "       108           9.5171           13.57m\n",
      "       109           9.4943           13.49m\n",
      "       110           9.4746           13.39m\n",
      "       111           9.4517           13.30m\n",
      "       112           9.4303           13.21m\n",
      "       113           9.4142           13.12m\n",
      "       114           9.3956           13.04m\n",
      "       115           9.3806           12.96m\n",
      "       116           9.3625           12.87m\n",
      "       117           9.3458           12.78m\n",
      "       118           9.3318           12.69m\n",
      "       119           9.3182           12.60m\n",
      "       120           9.3009           12.51m\n",
      "       121           9.2884           12.41m\n",
      "       122           9.2744           12.33m\n",
      "       123           9.2628           12.23m\n",
      "       124           9.2416           12.14m\n",
      "       125           9.2130           12.04m\n",
      "       126           9.1989           11.94m\n",
      "       127           9.1812           11.85m\n",
      "       128           9.1624           11.75m\n",
      "       129           9.1518           11.66m\n",
      "       130           9.1366           11.57m\n",
      "       131           9.1265           11.48m\n",
      "       132           9.0992           11.39m\n",
      "       133           9.0859           11.30m\n",
      "       134           9.0645           11.21m\n",
      "       135           9.0382           11.11m\n",
      "       136           9.0236           11.02m\n",
      "       137           9.0042           10.92m\n",
      "       138           8.9898           10.83m\n",
      "       139           8.9756           10.73m\n",
      "       140           8.9599           10.64m\n",
      "       141           8.9430           10.55m\n",
      "       142           8.9229           10.45m\n",
      "       143           8.9096           10.36m\n",
      "       144           8.8971           10.27m\n",
      "       145           8.8853           10.17m\n",
      "       146           8.8759           10.08m\n",
      "       147           8.8637            9.99m\n",
      "       148           8.8535            9.89m\n",
      "       149           8.8359            9.80m\n",
      "       150           8.8233            9.70m\n",
      "       151           8.8115            9.61m\n",
      "       152           8.8012            9.52m\n",
      "       153           8.7918            9.43m\n"
     ]
    }
   ],
   "source": [
    "model = pipeline.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['models/model-20210328-184456.pkl']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Saving the entire pipeline model as pkl file\n",
    "joblib.dump(pipeline, model_name, compress=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prediction and Evaluation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the model\n",
    "# model = joblib.load(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test RMSE: \n",
      "752.5397333333333\n",
      "\n",
      "\n",
      "Train RMSE: \n",
      "360.3723\n"
     ]
    }
   ],
   "source": [
    "predictions = model.predict(X_test).reshape(-1,1).astype(int).ravel()\n",
    "predictions_train = model.predict(X_train).reshape(-1,1).astype(int).ravel()\n",
    "\n",
    "print(\"Test RMSE: \")\n",
    "print(mean_squared_error(y_test, predictions))\n",
    "print(\"\\n\")\n",
    "print(\"Train RMSE: \")\n",
    "print(mean_squared_error(y_train, predictions_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train score:  0.9743364406283166\n"
     ]
    }
   ],
   "source": [
    "print(\"Train score: \", model.score(X_train, y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test score:  0.9464559196545642\n"
     ]
    }
   ],
   "source": [
    "print(\"Test score: \", model.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(30000,)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(30000,)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(target: np.array, predictions: np.array, thresh=3.) -> float:\n",
    "    diff = np.abs(target.ravel() - predictions.ravel()) <= thresh\n",
    "    acc = np.round((diff.sum()/diff.shape[0]) * 100, 2)\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Acc: 44.58%\n",
      "Test Acc: 18.77%\n"
     ]
    }
   ],
   "source": [
    "# Accuracy\n",
    "\n",
    "thresh = 3\n",
    "\n",
    "print(f\"Train Acc: {accuracy(y_train, predictions_train, thresh)}%\")\n",
    "\n",
    "print(f\"Test Acc: {accuracy(y_test, predictions, thresh)}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print((y_test.astype(int)[:15]))\n",
    "# print(predictions[:15])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf2",
   "language": "python",
   "name": "tf2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
